{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e1262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CocoPerClass(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco_det, coco_caps, dino_transform=None, clip_transform=None, binarize=True):\n",
    "        self.coco_det = coco_det\n",
    "        self.coco_caps = coco_caps\n",
    "        self.coco = coco_det.coco\n",
    "        self.dino_transform = dino_transform\n",
    "        self.clip_transform = clip_transform\n",
    "        self.binarize = binarize\n",
    "        \n",
    "        # Build flat index: [(img_idx, cat_id), ...]\n",
    "        self._flat_index = []\n",
    "        for img_idx, img_id in enumerate(coco_det.ids):\n",
    "            cat_ids = {ann[\"category_id\"] for ann in self.coco.imgToAnns[img_id]}\n",
    "            self._flat_index.extend((img_idx, cid) for cid in cat_ids)\n",
    "        \n",
    "        # ID to name mapping\n",
    "        self.cat_id2name = {\n",
    "            c[\"id\"]: c[\"name\"] for c in self.coco.loadCats(self.coco.getCatIds())\n",
    "        }\n",
    "        \n",
    "        # Build image_id to captions mapping\n",
    "        self.img_id_to_captions = {}\n",
    "        for i, img_id in enumerate(self.coco_caps.ids):\n",
    "            _, captions = self.coco_caps[i]\n",
    "            self.img_id_to_captions[img_id] = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flat_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, cat_id = self._flat_index[idx]\n",
    "        img, anns = self.coco_det[img_idx]\n",
    "        img_id = self.coco_det.ids[img_idx]\n",
    "        \n",
    "        # Keep original image\n",
    "        original_img = img.copy()\n",
    "        \n",
    "        # Build per-class mask\n",
    "        H, W = img.height, img.width\n",
    "        mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        \n",
    "        for ann in anns:\n",
    "            if ann[\"category_id\"] == cat_id:\n",
    "                ann_mask = self.coco.annToMask(ann).astype(bool)\n",
    "                mask[ann_mask] = 1 if self.binarize else cat_id\n",
    "        \n",
    "        # Convert mask to tensor (don't transform it)\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        \n",
    "        # Apply transforms to images only\n",
    "        dino_img = img.copy()\n",
    "        clip_img = img.copy()\n",
    "        \n",
    "        if self.dino_transform is not None:\n",
    "            dino_img = self.dino_transform(dino_img)\n",
    "        \n",
    "        if self.clip_transform is not None:\n",
    "            clip_img = self.clip_transform(clip_img)\n",
    "        \n",
    "        # Get captions for this image\n",
    "        captions = self.img_id_to_captions.get(img_id, [])\n",
    "        \n",
    "        return {\n",
    "            'dino_image': dino_img,\n",
    "            'clip_image': clip_img,\n",
    "            'image': original_img,\n",
    "            'category_id': cat_id,\n",
    "            'mask': mask,\n",
    "            'category_name': self.cat_id2name[cat_id],\n",
    "            'image_id': img_id,\n",
    "            'captions': captions,\n",
    "            'idx': idx\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    dino_images = torch.stack([item['dino_image'] for item in batch])\n",
    "    clip_images = torch.stack([item['clip_image'] for item in batch])\n",
    "    original_images = [item['image'] for item in batch]\n",
    "    masks = [item['mask'] for item in batch]\n",
    "    category_ids = [item['category_id'] for item in batch]\n",
    "    category_names = [item['category_name'] for item in batch]\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    captions_text = [item['captions'] for item in batch]\n",
    "    indices = [item['idx'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'dino_images': dino_images,\n",
    "        'clip_images': clip_images,\n",
    "        'original_images': original_images,\n",
    "        'masks': masks,\n",
    "        'category_ids': category_ids,\n",
    "        'category_names': category_names,\n",
    "        'image_ids': image_ids,\n",
    "        'captions_text': captions_text,\n",
    "        'indices': indices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb51700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/a_n29343/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/a_n29343/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/a_n29343/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/a_n29343/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import os, gc, h5py, torch, torchvision.transforms as T\n",
    "from tqdm.auto import tqdm\n",
    "import open_clip                                            \n",
    "from pathlib import Path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DINO_MODEL     = 'dinov2_vitl14_reg'          \n",
    "CLIP_MODEL     = 'hf-hub:laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K'\n",
    "DINO_TAG     = 'dinov2_vitl14_reg'          \n",
    "CLIP_TAG     = 'CLIP-ViT-L-14-DataComp'\n",
    "BATCH_SIZE     = 32\n",
    "\n",
    "IMAGENET_MEAN, IMAGENET_STD = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "class MaybeToTensor(T.ToTensor):\n",
    "    def __call__(self, pic):\n",
    "        return pic if isinstance(pic, torch.Tensor) else super().__call__(pic)\n",
    "\n",
    "normalize = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "dino_transform = T.Compose([\n",
    "    T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC), # original dino does it to 256 but it messes up the aspect ratio\n",
    "#     T.CenterCrop(224),\n",
    "    MaybeToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "model_clip, _, preprocess_clip = open_clip.create_model_and_transforms(CLIP_MODEL)\n",
    "\n",
    "transform_list = preprocess_clip.transforms\n",
    "clip_transform = T.Compose([T.Resize(size=(224, 224), interpolation=T.InterpolationMode.BICUBIC, antialias=True)] \n",
    "                            + transform_list[2:])\n",
    "\n",
    "clip_tokenizer = open_clip.get_tokenizer(CLIP_MODEL)\n",
    "\n",
    "model_clip = model_clip.to(device).eval()\n",
    "model_dino = torch.hub.load('facebookresearch/dinov2', DINO_MODEL).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499c207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS-SAE checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiStreamSparseAutoencoder(\n",
       "  (encoders): ModuleDict(\n",
       "    (dino): Linear(in_features=1024, out_features=8192, bias=False)\n",
       "    (clip_img): Linear(in_features=768, out_features=8192, bias=False)\n",
       "    (clip_txt): Linear(in_features=768, out_features=8192, bias=False)\n",
       "  )\n",
       "  (decoders): ModuleDict(\n",
       "    (dino): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "    (clip_img): Linear(in_features=8192, out_features=768, bias=False)\n",
       "    (clip_txt): Linear(in_features=8192, out_features=768, bias=False)\n",
       "  )\n",
       "  (pre_biases): ParameterDict(\n",
       "      (dino): Parameter containing: [torch.cuda.FloatTensor of size 1024 (cuda:0)]\n",
       "      (clip_img): Parameter containing: [torch.cuda.FloatTensor of size 768 (cuda:0)]\n",
       "      (clip_txt): Parameter containing: [torch.cuda.FloatTensor of size 768 (cuda:0)]\n",
       "  )\n",
       "  (latent_biases): ParameterDict(\n",
       "      (dino): Parameter containing: [torch.cuda.FloatTensor of size 8192 (cuda:0)]\n",
       "      (clip_img): Parameter containing: [torch.cuda.FloatTensor of size 8192 (cuda:0)]\n",
       "      (clip_txt): Parameter containing: [torch.cuda.FloatTensor of size 8192 (cuda:0)]\n",
       "  )\n",
       "  (gate): TopK(k=64)\n",
       "  (postact_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparc.model.model_global import MultiStreamSparseAutoencoder\n",
    "import json\n",
    "# Infer modality dimensions (needed to create MS-SAE skeleton)\n",
    "d_streams = {\n",
    "    'dino'     : model_dino.num_features,             # DINOv2 CLS/pool dim\n",
    "    'clip_img' : model_clip.visual.output_dim,\n",
    "    'clip_txt' : model_clip.visual.output_dim,\n",
    "}\n",
    "\n",
    "print('Loading MS-SAE checkpoint...')\n",
    "\n",
    "SAE_CHECKPOINT = Path('../../final_results/msae_open_global_with_cross/msae_checkpoint.pth')\n",
    "with open('../../final_results/msae_open_global_with_cross/run_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "msae = MultiStreamSparseAutoencoder(\n",
    "    d_streams=d_streams,\n",
    "    n_latents=config['args']['n_latents'],               # MUST match your training args\n",
    "    k=config['args']['k'],\n",
    ").to(device)\n",
    "msae.load_state_dict(torch.load(SAE_CHECKPOINT, map_location=device, weights_only=False))\n",
    "msae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6225a1",
   "metadata": {},
   "source": [
    "### Get the latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151e9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_latents(\n",
    "    coco_dataset,\n",
    "    model_dino,\n",
    "    model_clip,\n",
    "    msae,\n",
    "    clip_tokenizer,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def custom_collate(batch):\n",
    "        collated_batch = collate_fn(batch)\n",
    "        \n",
    "        first_captions = []\n",
    "        for cap_list in collated_batch['captions_text']:\n",
    "            first_captions.append(cap_list[0] if cap_list else \"\")\n",
    "        \n",
    "        if first_captions:\n",
    "            tokenized_captions = clip_tokenizer(first_captions)\n",
    "            collated_batch['tokenized_captions'] = tokenized_captions\n",
    "        \n",
    "        return collated_batch\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        coco_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=custom_collate,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(val_loader, desc=\"Extracting latents\"):\n",
    "            dino_images = batch_data['dino_images'].to(device)\n",
    "            clip_images = batch_data['clip_images'].to(device)\n",
    "            tokenized_captions = batch_data['tokenized_captions'].to(device)\n",
    "            image_ids = batch_data['image_ids']\n",
    "            captions_text_batch = batch_data['captions_text']\n",
    "\n",
    "            dino_features = model_dino(dino_images)\n",
    "            if isinstance(dino_features, tuple):\n",
    "                dino_features = dino_features[0]\n",
    "\n",
    "            clip_img_features = model_clip.encode_image(clip_images)\n",
    "            if isinstance(clip_img_features, tuple):\n",
    "                clip_img_features = clip_img_features[0]\n",
    "\n",
    "            clip_txt_features = model_clip.encode_text(tokenized_captions)\n",
    "\n",
    "            msae_inputs = {\n",
    "                'dino': dino_features,\n",
    "                'clip_img': clip_img_features,\n",
    "                'clip_txt': clip_txt_features\n",
    "            }\n",
    "\n",
    "            msae_outputs = msae(msae_inputs)\n",
    "\n",
    "            batch_latents_data = {}\n",
    "            for stream_name in d_streams.keys():\n",
    "                sparse_code_key = f'sparse_codes_{stream_name}'\n",
    "                if sparse_code_key in msae_outputs:\n",
    "                    batch_latents_data[stream_name] = msae_outputs[sparse_code_key].cpu().numpy()\n",
    "\n",
    "            for i in range(len(image_ids)):\n",
    "                sample_latents = {}\n",
    "                for stream_name, all_stream_latents_batch in batch_latents_data.items():\n",
    "                    sample_latents[stream_name] = all_stream_latents_batch[i]\n",
    "                \n",
    "                all_results.append({\n",
    "                    'image_id': image_ids[i],\n",
    "                    'captions': captions_text_batch[i],\n",
    "                    'category_id': batch_data['category_ids'][i],\n",
    "                    'category_name': batch_data['category_names'][i],\n",
    "                    'latents': sample_latents\n",
    "                })\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d355169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.60s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CocoDetection, CocoCaptions\n",
    "from torchvision import transforms\n",
    "\n",
    "img_dir  = \"../../dataset/COCO/val2017\"\n",
    "det_ann_file = \"../../dataset/COCO/annotations/instances_val2017.json\"\n",
    "cap_ann_file = \"../../dataset/COCO/annotations/captions_val2017.json\"\n",
    "\n",
    "coco_det = CocoDetection(root=img_dir, annFile=det_ann_file)\n",
    "coco_caps = CocoCaptions(root=img_dir, annFile=cap_ann_file)\n",
    "dataset = CocoPerClass(coco_det, coco_caps, dino_transform, clip_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38c34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea559a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../../final_results/\", exist_ok=True)\n",
    "\n",
    "latent_file_path = '../../final_results/segmentation/coco_latents_global.pickle'\n",
    "if os.path.isfile(latent_file_path):\n",
    "    with open(latent_file_path, 'rb') as f:\n",
    "        all_latents = pickle.load(f)\n",
    "else:\n",
    "    all_latents = get_coco_latents(dataset, model_dino, model_clip, msae, clip_tokenizer)\n",
    "    with open(latent_file_path, 'wb') as f:\n",
    "        pickle.dump(all_latents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9fe32ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_class_file_path = '../../final_results/segmentation/coco_latents_count_per_class_global.pickle'\n",
    "if os.path.isfile(latent_class_file_path):\n",
    "    with open(latent_class_file_path, 'rb') as f:\n",
    "        latent_class_count = pickle.load(f)\n",
    "else:\n",
    "    with open(latent_class_file_path, 'wb') as f:\n",
    "        num_latents = len(all_latents[0]['latents']['dino'])\n",
    "        num_classes = max(dataset.cat_id2name.keys())+1\n",
    "        num_streams = len(all_latents[0]['latents'])\n",
    "        latent_class_count = np.zeros((num_streams, num_latents, num_classes))\n",
    "\n",
    "        for i in tqdm(range(len(all_latents))):\n",
    "            for stream_idx, stream in enumerate(all_latents[i]['latents']):\n",
    "                active_idx = np.where(all_latents[i]['latents'][stream]>0)[0]\n",
    "                class_id = dataset[i]['category_id']\n",
    "                latent_class_count[stream_idx, active_idx, class_id] += 1\n",
    "        pickle.dump(latent_class_count, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abab73",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16cfb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sparc.heatmaps.attention_relevance import interpret_sparc, interpret_clip\n",
    "from sparc.heatmaps.gradcam import compute_gradcam\n",
    "from sparc.heatmaps.visualization import (show_clean_text_attribution, \n",
    "                                    plot_all_attributions, \n",
    "                                    show_clean_gradcam_text,  \n",
    "                                    plot_relevancy_attributions)\n",
    "\n",
    "from sparc.heatmaps.attention_relevance import get_all_latents\n",
    "from sparc.heatmaps.attention_relevance import get_attention_blocks, compute_attention_relevancy\n",
    "from sparc.heatmaps.clip import create_wrapped_clip, patch_clip_keep_last\n",
    "from sparc.heatmaps.dino import create_wrapped_dinov2, patch_dinov2_keep_last\n",
    "from sparc.heatmaps.attention_relevance import compute_attention_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819c737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(word):\n",
    "    return \"An\" if word[0].lower() in 'aeiou' else \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26098130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_mask(image_relevance):\n",
    "    batch_size, spatial_size = image_relevance.shape\n",
    "    dim = int(spatial_size ** 0.5)\n",
    "    image_relevance = image_relevance.reshape(batch_size, 1, dim, dim)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.squeeze(1).cuda().data.cpu().numpy()\n",
    "    flat = image_relevance.reshape(batch_size, -1)\n",
    "    return ((flat - flat.min(1, keepdims=True)) \n",
    "            / (flat.max(1, keepdims=True) - flat.min(1, keepdims=True))).reshape(batch_size, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f67626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/a_n29343/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "model_clip = create_wrapped_clip(CLIP_MODEL, device)\n",
    "model_clip = model_clip.enable_attention_capture()\n",
    "patch_clip_keep_last(model_clip, last_layer=23)\n",
    "\n",
    "model_dino = create_wrapped_dinov2(DINO_MODEL, device)\n",
    "model_dino.enable_attention_capture()\n",
    "patch_dinov2_keep_last(model_dino, last_layer=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b3e65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_msae = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f454d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import Tuple\n",
    "def interpret_clip_sim(\n",
    "    image: torch.Tensor, \n",
    "    texts: torch.Tensor, \n",
    "    model: nn.Module, \n",
    "    device: str, \n",
    "    start_layer: int = -1, \n",
    "    start_layer_text: int = -1\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Interpret CLIP model using attention rollout.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image tensor\n",
    "        texts: Input text tokens\n",
    "        model: CLIP model\n",
    "        device: Device to use\n",
    "        start_layer: Starting layer for image attention rollout\n",
    "        start_layer_text: Starting layer for text attention rollout\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (text_relevance, image_relevance)\n",
    "    \"\"\"\n",
    "    batch_size = texts.shape[0]\n",
    "    images = image.repeat(batch_size, 1, 1, 1)\n",
    "    # Forward\n",
    "    image_features = model.encode_image(images)\n",
    "    text_features = model.encode_text(texts)\n",
    "\n",
    "    # Normalized features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Cosine similarity as logits\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    \n",
    "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "    index = [i for i in range(batch_size)]\n",
    "    one_hot = np.zeros((logits_per_image.shape[0], logits_per_image.shape[1]), dtype=np.float32)\n",
    "    one_hot[torch.arange(logits_per_image.shape[0]), index] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    one_hot = torch.sum(one_hot.cuda() * logits_per_image)\n",
    "    model.zero_grad()\n",
    "\n",
    "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
    "\n",
    "    if start_layer == -1:\n",
    "        start_layer = len(image_attn_blocks) - 1\n",
    "    \n",
    "    image_relevance = compute_attention_relevancy(\n",
    "        one_hot, image_attn_blocks, device, batch_size, start_layer\n",
    "    )\n",
    "    image_relevance = image_relevance[:, 0, 1:]\n",
    "    return image_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4cffd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f572c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_np(np_array):\n",
    "    return Image.fromarray(np.uint8(np_array*255)).resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503900d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"Recursively convert numpy types to Python native types\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_segmentation_data(data, filepath):\n",
    "    \"\"\"Save complete segmentation data for all methods\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    # Convert numpy types to Python native types\n",
    "    clean_data = convert_numpy_types(data)\n",
    "    \n",
    "    # Add metadata\n",
    "    save_data = {\n",
    "        'methods': list(data.keys()),\n",
    "        'metadata': {\n",
    "            'iou_threshold': float(IOU_THR),\n",
    "            'fallback_threshold': float(THR_FALLBACK),\n",
    "            'total_methods': len(data.keys())\n",
    "        },\n",
    "        'data': clean_data\n",
    "    }\n",
    "    \n",
    "    print(f\"Saving segmentation data to {filepath}...\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(save_data, f, indent=2)\n",
    "    print(\"✓ Segmentation data saved!\")\n",
    "\n",
    "def load_segmentation_data(filepath):\n",
    "    \"\"\"Load complete segmentation data for all methods\"\"\"\n",
    "    print(f\"Loading segmentation data from {filepath}...\")\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        save_data = json.load(f)\n",
    "    \n",
    "    data = save_data['data']\n",
    "    metadata = save_data.get('metadata', {})\n",
    "    \n",
    "    print(f\"✓ Loaded data for {len(data)} methods: {list(data.keys())}\")\n",
    "    for method in data.keys():\n",
    "        print(f\"  - {method}: {len(data[method]['images'])} images, \"\n",
    "              f\"{len(data[method]['anns_dt'])} predictions\")\n",
    "    \n",
    "    return data, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa21c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, cv2, torch\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "from pycocotools import mask as mu\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "IOU_THR = 0.20\n",
    "THR_FALLBACK = 0.30\n",
    "\n",
    "def rle(arr):\n",
    "    \"\"\"Convert uint8 H×W array to RLE dict (json-safe)\"\"\"\n",
    "    r = mu.encode(np.asfortranarray(arr))\n",
    "    r[\"counts\"] = r[\"counts\"].decode(\"ascii\")\n",
    "    return r\n",
    "\n",
    "def batch_resize(t, size_hw):\n",
    "    return torch.nn.functional.interpolate(\n",
    "        t, size=size_hw, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# Define all methods\n",
    "methods = ['dino_sparc_sum', 'clip_sparc_sum', 'clip_sparc_cross', 'dino_sparc_cross', 'clip_sim']\n",
    "\n",
    "# Initialize data structures for all methods\n",
    "data = {}\n",
    "for method in methods:\n",
    "    data[method] = {\n",
    "        'images': [], \n",
    "        'anns_gt': [], \n",
    "        'anns_dt': [],\n",
    "        'cls2id': {},\n",
    "        'ann_id': 1\n",
    "    }\n",
    "\n",
    "# Main processing loop\n",
    "for i, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    dino_imgs = batch['dino_images'].to(device)\n",
    "    clip_imgs = batch['clip_images'].to(device)\n",
    "    masks = batch['masks']\n",
    "    cls_names = batch['category_names']\n",
    "    img_ids = batch['image_ids']\n",
    "    \n",
    "    # Get latents\n",
    "    sparc_texts = [f'{i}' for i in cls_names]\n",
    "    tokenized_sparc_texts = clip_tokenizer(sparc_texts).to(device)\n",
    "    \n",
    "    clip_txt_latent, clip_img_latent, dino_latent, batch_size = get_all_latents(\n",
    "        tokenized_sparc_texts, msae, model_dino, dino_imgs, model_clip, clip_imgs, global_msae\n",
    "    )\n",
    "    \n",
    "    # Compute targets\n",
    "    class_id = batch['category_ids']\n",
    "\n",
    "    class_count_per_latent = latent_class_count.sum(0)\n",
    "    class_latents = np.where(class_count_per_latent[:, class_id]>50)[0]\n",
    "    \n",
    "    targets = {\n",
    "        'dino_sparc_sum': torch.sum(dino_latent[:, class_latents]),\n",
    "        'clip_sparc_sum': torch.sum(clip_img_latent[:, class_latents]),\n",
    "        'clip_sparc_cross': clip_txt_latent @ clip_img_latent.T,\n",
    "        'dino_sparc_cross': clip_txt_latent @ dino_latent.T\n",
    "    }\n",
    "    \n",
    "    model_clip.zero_grad()\n",
    "    model_dino.zero_grad()\n",
    "    \n",
    "    # Get attention blocks\n",
    "    clip_img_attn_blocks, clip_txt_attn_blocks = get_attention_blocks(model_clip, 'clip')\n",
    "    dino_img_attn_blocks, _ = get_attention_blocks(model_dino, 'dino')\n",
    "    \n",
    "    # Compute relevancies and masks\n",
    "    masks_dict = {}\n",
    "    \n",
    "    # SPARC methods\n",
    "    for method in ['dino_sparc_sum', 'dino_sparc_cross']:\n",
    "        relevance = compute_attention_relevancy(\n",
    "            targets[method], dino_img_attn_blocks, device, batch_size, start_layer=23\n",
    "        )\n",
    "        relevance = relevance[:, 0, 5:]  # Skip CLS + register tokens\n",
    "        masks_dict[method] = get_image_mask(relevance)\n",
    "    \n",
    "    for method in ['clip_sparc_sum', 'clip_sparc_cross']:\n",
    "        relevance = compute_attention_relevancy(\n",
    "            targets[method], clip_img_attn_blocks, device, batch_size, start_layer=23\n",
    "        )\n",
    "        relevance = relevance[:, 0, 1:]  # Skip CLS token\n",
    "        masks_dict[method] = get_image_mask(relevance)\n",
    "    \n",
    "    # CLIP SIM method\n",
    "    clip_sim_relevance = interpret_clip_sim(clip_imgs, tokenized_sparc_texts, model_clip, device)\n",
    "    masks_dict['clip_sim'] = get_image_mask(clip_sim_relevance)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    for method in methods:\n",
    "        if isinstance(masks_dict[method], torch.Tensor):\n",
    "            masks_dict[method] = masks_dict[method].detach().cpu().numpy()\n",
    "    \n",
    "    B = len(img_ids)\n",
    "    \n",
    "    # Process each sample\n",
    "    for b in range(B):\n",
    "        img_id = img_ids[b]\n",
    "        gt_mask = masks[b]\n",
    "        H, W = gt_mask.shape\n",
    "        cls = cls_names[b]\n",
    "\n",
    "        if gt_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Prepare ground truth (same for all methods)\n",
    "        gt_bin = (gt_mask > 0).numpy().astype(np.uint8)\n",
    "        g_rle = rle(gt_bin)\n",
    "        ys, xs = np.where(gt_bin)\n",
    "        x0, x1, y0, y1 = xs.min(), xs.max()+1, ys.min(), ys.max()+1\n",
    "        \n",
    "        # Process all methods\n",
    "        for method in methods:\n",
    "            # Add ground truth\n",
    "            cid = data[method]['cls2id'].setdefault(cls, len(data[method]['cls2id'])+1)\n",
    "            data[method]['images'].append({\"id\": img_id, \"width\": W, \"height\": H})\n",
    "            data[method]['anns_gt'].append({\n",
    "                \"id\": data[method]['ann_id'], \"image_id\": img_id, \"category_id\": cid,\n",
    "                \"segmentation\": g_rle, \"area\": int(mu.area(g_rle)),\n",
    "                \"bbox\": [x0,y0,x1-x0,y1-y0], \"iscrowd\": 0\n",
    "            })\n",
    "            data[method]['ann_id'] += 1\n",
    "            \n",
    "            # Process prediction\n",
    "            h = cv2.resize(masks_dict[method][b], (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "            if np.isnan(h).any():\n",
    "                continue\n",
    "            thr = threshold_otsu(h)\n",
    "            m = h >= thr\n",
    "            if m.sum() == 0:\n",
    "                m = h >= (h.max() * THR_FALLBACK)\n",
    "            if m.sum() > 0:\n",
    "                m_np = m.astype(np.uint8)\n",
    "                d_rle = rle(m_np)\n",
    "                ys, xs = np.where(m_np)\n",
    "                x0, x1 = xs.min(), xs.max() + 1\n",
    "                y0, y1 = ys.min(), ys.max() + 1\n",
    "                data[method]['anns_dt'].append({\n",
    "                    \"image_id\": img_id, \"category_id\": cid, \"score\": 1.0,\n",
    "                    \"segmentation\": d_rle, \"bbox\": [x0, y0, x1 - x0, y1 - y0]\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a5d8b8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# os.makedirs(\"../../final_results/\", exist_ok=True)\n",
    "# save_segmentation_data(data, '../../final_results/segmentation/segmentation_result_latent_global.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8f74d38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate all methods\n",
    "def evaluate_all_methods(data, methods):\n",
    "    results = {}\n",
    "\n",
    "    for method in methods:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"EVALUATING {method.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        cats = [{'id': v, 'name': k} for k, v in data[method]['cls2id'].items()]\n",
    "        gt = {'images': data[method]['images'], 'annotations': data[method]['anns_gt'], 'categories': cats}\n",
    "\n",
    "        print(f\"{method}: {len(data[method]['images'])} images, {len(data[method]['anns_dt'])} predictions\")\n",
    "\n",
    "        coco_gt = COCO()\n",
    "        coco_gt.dataset = gt\n",
    "        coco_gt.createIndex()\n",
    "        coco_dt = coco_gt.loadRes(data[method]['anns_dt'])\n",
    "\n",
    "        eval_obj = COCOeval(coco_gt, coco_dt, iouType=\"segm\")\n",
    "        eval_obj.params.iouThrs = np.array([IOU_THR])\n",
    "        eval_obj.evaluate()\n",
    "        eval_obj.accumulate()\n",
    "        eval_obj.summarize()\n",
    "\n",
    "        results[method] = eval_obj.stats[0]\n",
    "\n",
    "    # Comparison\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for method, score in results.items():\n",
    "        print(f\"{method} mAP@{IOU_THR}: {score:.4f}\")\n",
    "\n",
    "    print(\"\\nRankings:\")\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    for rank, (method, score) in enumerate(sorted_results, 1):\n",
    "        print(f\"{rank}. {method}: {score:.4f}\")\n",
    "\n",
    "    best_method, best_score = sorted_results[0]\n",
    "    print(f\"\\nWinner: {best_method} with {best_score:.4f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd2c7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading segmentation data from final_results/segmentation/segmentation_result_latent_global.json...\n",
      "✓ Loaded data for 5 methods: ['dino_sparc_sum', 'clip_sparc_sum', 'clip_sparc_cross', 'dino_sparc_cross', 'clip_sim']\n",
      "  - dino_sparc_sum: 14631 images, 14541 predictions\n",
      "  - clip_sparc_sum: 14631 images, 14523 predictions\n",
      "  - clip_sparc_cross: 14631 images, 14631 predictions\n",
      "  - dino_sparc_cross: 14631 images, 14631 predictions\n",
      "  - clip_sim: 14631 images, 14631 predictions\n"
     ]
    }
   ],
   "source": [
    "data, metadata = load_segmentation_data('../../final_results/segmentation/segmentation_result_latent_global.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "386980b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING DINO_SPARC_SUM\n",
      "============================================================\n",
      "dino_sparc_sum: 14631 images, 14541 predictions\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=4.96s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.61s).\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.222\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.244\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=  1 ] = 0.352\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets= 10 ] = 0.352\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.352\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.284\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.516\n",
      "============================================================\n",
      "EVALUATING CLIP_SPARC_SUM\n",
      "============================================================\n",
      "clip_sparc_sum: 14631 images, 14523 predictions\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=5.93s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.65s).\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.222\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.276\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.254\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=  1 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets= 10 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.310\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.493\n",
      "============================================================\n",
      "EVALUATING CLIP_SPARC_CROSS\n",
      "============================================================\n",
      "clip_sparc_cross: 14631 images, 14631 predictions\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=4.94s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.65s).\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.223\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.283\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=  1 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets= 10 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.019\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.569\n",
      "============================================================\n",
      "EVALUATING DINO_SPARC_CROSS\n",
      "============================================================\n",
      "dino_sparc_cross: 14631 images, 14631 predictions\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=5.69s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.62s).\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.222\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.253\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.274\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=  1 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets= 10 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.297\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.581\n",
      "============================================================\n",
      "EVALUATING CLIP_SIM\n",
      "============================================================\n",
      "clip_sim: 14631 images, 14631 predictions\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=5.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.60s).\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.248\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.370\n",
      " Average Precision  (AP) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.282\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=  1 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets= 10 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=   all | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= small | maxDets=100 ] = 0.050\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area=medium | maxDets=100 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.20:0.20 | area= large | maxDets=100 ] = 0.625\n",
      "============================================================\n",
      "COMPARISON SUMMARY\n",
      "============================================================\n",
      "dino_sparc_sum mAP@0.2: 0.2218\n",
      "clip_sparc_sum mAP@0.2: 0.2225\n",
      "clip_sparc_cross mAP@0.2: 0.2230\n",
      "dino_sparc_cross mAP@0.2: 0.2222\n",
      "clip_sim mAP@0.2: 0.2477\n",
      "\n",
      "Rankings:\n",
      "1. clip_sim: 0.2477\n",
      "2. clip_sparc_cross: 0.2230\n",
      "3. clip_sparc_sum: 0.2225\n",
      "4. dino_sparc_cross: 0.2222\n",
      "5. dino_sparc_sum: 0.2218\n",
      "\n",
      "Winner: clip_sim with 0.2477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dino_sparc_sum': 0.22183031405480547,\n",
       " 'clip_sparc_sum': 0.22248569137634533,\n",
       " 'clip_sparc_cross': 0.2229675861393922,\n",
       " 'dino_sparc_cross': 0.22223839494804298,\n",
       " 'clip_sim': 0.24770786555228405}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_all_methods(data, list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fa146",
   "metadata": {},
   "source": [
    "### mIOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycocotools import mask as mu\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_iou(mask1_rle, mask2_rle):\n",
    "    \"\"\"Compute IoU between two RLE masks\"\"\"\n",
    "    # Decode RLE to binary masks\n",
    "    mask1 = mu.decode(mask1_rle)\n",
    "    mask2 = mu.decode(mask2_rle)\n",
    "    \n",
    "    # Compute intersection and union\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def compute_miou(data, method):\n",
    "    \"\"\"Compute mIOU for a specific method\"\"\"\n",
    "    print(f\"Computing mIOU for {method}...\")\n",
    "    \n",
    "    # Group ground truth and predictions by image_id and category_id\n",
    "    gt_by_img_cat = {}\n",
    "    dt_by_img_cat = {}\n",
    "    \n",
    "    # Index ground truth\n",
    "    for gt_ann in data[method]['anns_gt']:\n",
    "        key = (gt_ann['image_id'], gt_ann['category_id'])\n",
    "        gt_by_img_cat[key] = gt_ann\n",
    "    \n",
    "    # Index predictions  \n",
    "    for dt_ann in data[method]['anns_dt']:\n",
    "        key = (dt_ann['image_id'], dt_ann['category_id'])\n",
    "        dt_by_img_cat[key] = dt_ann\n",
    "    \n",
    "    # Compute IoU for all ground truth annotations (penalize missing predictions)\n",
    "    ious = []\n",
    "    matched_pairs = 0\n",
    "    \n",
    "    for key, gt_ann in gt_by_img_cat.items():  # Changed: iterate through all GT\n",
    "        if key in dt_by_img_cat:\n",
    "            gt_mask = gt_ann['segmentation']\n",
    "            dt_mask = dt_by_img_cat[key]['segmentation']\n",
    "            \n",
    "            iou = compute_iou(gt_mask, dt_mask)\n",
    "            ious.append(iou)\n",
    "            matched_pairs += 1\n",
    "        else:\n",
    "            # Missing prediction: penalize with IoU = 0\n",
    "            ious.append(0.0)\n",
    "    \n",
    "    if len(ious) == 0:\n",
    "        return 0.0, 0\n",
    "    \n",
    "    miou = np.mean(ious)\n",
    "    print(f\"  {method}: {matched_pairs} matched pairs, mIOU = {miou:.4f}\")\n",
    "    \n",
    "    return miou, matched_pairs\n",
    "\n",
    "def compute_all_miou(data):\n",
    "    \"\"\"Compute mIOU for all methods\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPUTING mIOU FOR ALL METHODS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method in data.keys():\n",
    "        miou, matched_pairs = compute_miou(data, method)\n",
    "        results[method] = {\n",
    "            'mIOU': miou,\n",
    "            'matched_pairs': matched_pairs\n",
    "        }\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"mIOU COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for method, result in results.items():\n",
    "        print(f\"{method}: mIOU = {result['mIOU']:.4f} ({result['matched_pairs']} pairs)\")\n",
    "    \n",
    "    # Rankings\n",
    "    print(\"\\nRankings by mIOU:\")\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mIOU'], reverse=True)\n",
    "    for rank, (method, result) in enumerate(sorted_results, 1):\n",
    "        print(f\"{rank}. {method}: {result['mIOU']:.4f}\")\n",
    "    \n",
    "    best_method, best_result = sorted_results[0]\n",
    "    print(f\"\\nBest mIOU: {best_method} with {best_result['mIOU']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cecab2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPUTING mIOU FOR ALL METHODS\n",
      "============================================================\n",
      "Computing mIOU for dino_sparc_sum...\n",
      "  dino_sparc_sum: 14541 matched pairs, mIOU = 0.1368\n",
      "Computing mIOU for clip_sparc_sum...\n",
      "  clip_sparc_sum: 14523 matched pairs, mIOU = 0.1307\n",
      "Computing mIOU for clip_sparc_cross...\n",
      "  clip_sparc_cross: 14631 matched pairs, mIOU = 0.1381\n",
      "Computing mIOU for dino_sparc_cross...\n",
      "  dino_sparc_cross: 14631 matched pairs, mIOU = 0.1429\n",
      "Computing mIOU for clip_sim...\n",
      "  clip_sim: 14631 matched pairs, mIOU = 0.1567\n",
      "\n",
      "========================================\n",
      "mIOU COMPARISON\n",
      "========================================\n",
      "dino_sparc_sum: mIOU = 0.1368 (14541 pairs)\n",
      "clip_sparc_sum: mIOU = 0.1307 (14523 pairs)\n",
      "clip_sparc_cross: mIOU = 0.1381 (14631 pairs)\n",
      "dino_sparc_cross: mIOU = 0.1429 (14631 pairs)\n",
      "clip_sim: mIOU = 0.1567 (14631 pairs)\n",
      "\n",
      "Rankings by mIOU:\n",
      "1. clip_sim: 0.1567\n",
      "2. dino_sparc_cross: 0.1429\n",
      "3. clip_sparc_cross: 0.1381\n",
      "4. dino_sparc_sum: 0.1368\n",
      "5. clip_sparc_sum: 0.1307\n",
      "\n",
      "Best mIOU: clip_sim with 0.1567\n"
     ]
    }
   ],
   "source": [
    "miou_results = compute_all_miou(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b867fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
